---
title: 'Feedforward and Backward Propagation in Gated Recurrent Unit (GRU)'
date: 2019-12-09
permalink: /posts/2019/12/gru-formula/
tags:
  - deep learning
  - RNN
  - derivation
---

In this post, I'll discuss how to implement a simple Recurrent Neural Network (RNN), specifically the Gated Recurrent Unit (GRU). I'll present the feed forward proppagation of a GRU Cell at a single time stamp and then derive the formulas for deriving paprameter gradients using the concept of Backpropagation through time (BPTT). 
<p align="center">
<img src='/images/blog/GRU.PNG'>
</p>

Forward Propagation
======
The equations for a GRU cell looks like the following:

$$z_t = \sigma(W_{zh}\ast h_{t-1} + W_{zx}\ast x_t)$$

$$r_t = \sigma(W_{rh}\ast h_{t-1} + W_{rx}\ast x_t)$$

$$\tilde{h}_t = f(W_h\ast (r_t \circ h_{t-1}) + W_x\ast x_t)$$

$$h_t = (1-z_t)\circ \tilde{h}_t +  z_t\circ h_{t-1},$$

where $x_t$ is the input vector at time $t$, $h_t$ is the output vector, $\ast$ denotes matrix product, $\circ$ denotes element-wise product, $\sigma$ and $f$ are the sigmoid and Tanh activation functions, respectively.

Backward Propagation
======
Lets rewrite these set of equation in terms of unary and binary operations and in the order in which they must be computed.

$$
g_1 = W_{zh} \ast h_{t-1}\\
g_2 = W_{zx} \ast x_{t}
$$
