---
title: 'Feedforward and Backward Propagation in Gated Recurrent Unit (GRU)'
date: 2019-12-09
permalink: /posts/2019/12/gru-formula/
tags:
  - deep learning
  - RNN
  - derivation
---

In this post, I'll discuss how to implement a simple Recurrent Neural Network (RNN), specifically the Gated Recurrent Unit (GRU). I'll present the feed forward proppagation of a GRU Cell at a single time stamp and then derive the formulas for deriving paprameter gradients using the concept of Backpropagation through time (BPTT). 
<p align="center">
<img src='/images/blog/GRU.PNG'>
</p>

Forward Propagation
======
When $$a \neq 0$$, there are two solutions to $ax^2 + bx + c = 0$ and they are<br/>
$$ x = {-b \pm \sqrt{b^2-4ac} \over 2a} $$

Backward Propagation
======

