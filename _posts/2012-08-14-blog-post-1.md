---
title: 'Feedforward and Backward Propagation in Gated Recurrent Unit (GRU)'
date: 2019-12-09
permalink: /posts/2019/12/gru-formula/
tags:
  - deep learning
  - RNN
  - derivation
---

In this post, I'll discuss how to implement a simple Recurrent Neural Network (RNN), specifically the Gated Recurrent Unit (GRU). I'll present the feed forward proppagation of a GRU Cell at a single time stamp and then derive the formulas for deriving paprameter gradients using the concept of Backpropagation through time (BPTT). 
<p align="center">
<img src='/images/blog/GRU.PNG'>
</p>

Forward Propagation
======
The equations for a GRU cell looks like the following:

$$z_t = \sigma(W_{zh}\ast h_{t-1} + W_{zx}\ast x_t)$$

$$r_t = \sigma(W_{rh}\ast h_{t-1} + W_{rx}\ast x_t)$$

$$\tilde{h}_t = f(W_h\ast (r_t \circ h_{t-1}) + W_x\ast x_t)$$

$$h_t = (1-z_t)\circ \tilde{h}_t  z_t\circ h_t$$

When $a \neq 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are

$$ x = {-b \pm \sqrt{b^2-4ac} \over 2a} $$

Backward Propagation
======

